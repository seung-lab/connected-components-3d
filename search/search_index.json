{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#cc3d-connected-components-on-multilabel-3d-images","title":"cc3d: Connected Components on Multilabel 3D Images","text":"<p> Fig. 1. Binary and Multilabel Connected Components Labeling (CCL) 2D images are shown for simplicity. Black is the background color (zero). (a) A binary image (foreground white, background black) (b) 4-connected CCL of binary image (c) 8-connected CCL of binary image (d) A multilabel image (e) 4-connected CCL of multilabel image (f) 8-connected CCL of multilabel image. </p> <p> Fig. 2. Continuous Value Connected Components Labeling (CCL) (top) A three tone grayscale image with signed additive low magnitude noise (bottom) Extracted components using continuous value CCL with a delta value greater than the noise magnitude but smaller than the difference between tones </p> <p>cc3d is an implementation of connected components in three dimensions using a 26, 18, or 6-connected neighborhood in 3D or 4 and 8-connected in 2D. This package uses a 3D variant of the two pass method by Rosenfeld and Pflatz augmented with Union-Find and a decision tree based on the 2D 8-connected work of Wu, Otoo, and Suzuki. This implementation is compatible with images containing many different labels, not just binary images. It also supports continuously valued images such as grayscale microscope images with an algorithm that joins together nearby values.</p> <p>I wrote this package because I was working on densely labeled 3D biomedical images of brain tissue (e.g. 512x512x512 voxels). Other off the shelf implementations I reviewed were limited to binary images. This rendered these other packages too slow for my use case as it required masking each label and running the connected components algorithm once each time. For reference, there are often between hundreds to thousands of labels in a given volume. The benefit of this package is that it labels all connected components in one shot, improving performance by one or more orders of magnitude.</p> <p>In general, binary images are much more common (usually resulting from image thresholding), but multi-label images crop up in instance segmentation and semantic labeling as a classifier may label touching clusters of adjacent pixels differently. If a gap between different labels is guaranteed, then the problem degenerates into the binary version.</p> <p>Check out benchmarks to see a comparison with SciPy on a few different tasks.</p>"},{"location":"#python-pip-installaction","title":"Python <code>pip</code> Installaction","text":"<p>If compatible binaries are available for your platform, installation is particularly simple.</p> <pre><code>pip install connected-components-3d\n</code></pre> <p>If compatible binaries are not available, you can install from source as follows.</p> <p>Requires a C++ compiler.</p> <pre><code>pip install numpy\npip install connected-components-3d --no-binary :all:\n</code></pre> <p>Occasionally, you may appear to successfully install cc3d, but on import you'll see an error that includes: <code>numpy.ufunc size changed, may indicate binary incompatibility</code>. You can either try upgrading numpy or compiling cc3d from source in this case.</p>"},{"location":"#python-manual-installation","title":"Python Manual Installation","text":"<p>Requires a C++ compiler.</p> <pre><code>pip install -r requirements.txt\npython setup.py develop\n</code></pre>"},{"location":"#python-use","title":"Python Use","text":"<p>For detailed documentation, see the API docs.</p> <p>The following functions are available with examples below:</p> <ul> <li>Connected Component Labeling (CCL)</li> <li>Calculating centroids, bounding boxes, and voxel counts</li> <li>Removal of small objects (\"dust\") (or large objects)</li> <li>Extraction of k largest objects</li> <li>Fast extraction of all objects one-by-one</li> <li>Calculation of contact surface area and contact network</li> <li>Extraction and coloring of a per voxel connectivity graph</li> </ul> <pre><code>import cc3d\nimport numpy as np\n\nlabels_in = np.ones((512, 512, 512), dtype=np.int32)\nlabels_out = cc3d.connected_components(labels_in) # 26-connected\n\nconnectivity = 6 # only 4,8 (2D) and 26, 18, and 6 (3D) are allowed\nlabels_out = cc3d.connected_components(labels_in, connectivity=connectivity)\n\n# By default, cc3d works on multivalued labelings, but sometimes you want\n# to treat a grayscale image as a binary image directly. It is also possible\n# to process binary images more effectively. Binary image specific optimizations\n# are not implemented yet though, but may be in the future.\nlabels_out = cc3d.connected_components(labels_in, binary_image=True)\n# same as above, but less efficient\nlabels_out = cc3d.connected_components(labels_in &gt; 0)\n\n# If you need the borders to wrap around (e.g. for simulations, world maps)\n# specify periodic_boundary=True, currently only supported for\n# 4 and 8 (2d) and 6 (3d) connectivities.\nlabels_out = cc3d.connected_components(\n  labels_in, connectivity=connectivity, periodic_boundary=True\n)\n\n# If you need a particular dtype you can specify np.uint16, np.uint32, or np.uint64\n# You can go bigger, not smaller, than the default which is selected\n# to be the smallest that can be safely used. This can save you the copy\n# operation needed by labels_out.astype(...).\nlabels_out = cc3d.connected_components(labels_in, out_dtype=np.uint64)\n\n# If you're working with continuously valued images like microscopy\n# images you can use cc3d to perform a very rough segmentation.\n# If delta = 0, standard high speed processing. If delta &gt; 0, then\n# neighbor voxel values &lt;= delta are considered the same component.\n# The algorithm can be 2-10x slower though. Zero is considered\n# background and will not join to any other voxel.\nlabels_out = cc3d.connected_components(labels_in, delta=10)\n\n# If you're working with an image that's larger than memory you can\n# use mmapped files. The input and output files can be used independently.\n# In this case an array labels.bin that is 5000x5000x2000 voxels and uint32_t\n# in Fortran order is computed and the results are written to out.bin in Fortran\n# order. You can find the properties of the file (shape, dtype, order) by inspecting\n# labels_out.\nlabels_in = np.memmap(\"labels.bin\", order=\"F\", dtype=np.uint32, shape=(5000, 5000, 2000))\nlabels_out = cc3d.connected_components(labels_in, out_file=\"out.bin\")\n\n# Here's another strategy that you can use for huge files that won't even\n# take up any disk space. Provide any iterator to this function that produces\n# thick z sections of the input array that are in sequential order.\n# The output is a highly compressed CrackleArray that is still random access.\n# See: https://github.com/seung-lab/crackle\n# You need to pip install connected-components-3d[stack] to get the extra modules.\ndef sections(labels_in):\n  \"\"\"\n  A generator that produces thick Z slices\n  of an image\n  \"\"\"\n  for z in range(0, labels_in.shape[2], 100):\n    yield labels_in[:,:,z:z+100]\n\n# You can access compressed_labels_out using array notation\ncompressed_labels_out = cc3d.connected_components_stack(sections(labels))\n# convert to numpy array, probably a big mistake since\n# you probably expected it was going to blow up RAM\ncc_labels = compressed_labels_out.numpy()\n# if you don't like hanging onto this exotic format, you\n# can write it as a numpy array to disk in a memory efficient way.\ncompressed_labels_out.save(\"example.npy.gz\")\n# or hang onto it\ncompressed_labels_out.save(\"example.ckl\")\n\n\n# You can extract the number of labels (which is also the maximum\n# label value) like so:\nlabels_out, N = cc3d.connected_components(labels_in, return_N=True) # free\n# -- OR --\nlabels_out = cc3d.connected_components(labels_in)\nN = np.max(labels_out) # costs a full read\n\n# You can extract individual components using numpy operators\n# This approach is slow, but makes a mutable copy.\nfor segid in range(1, N+1):\n  extracted_image = labels_out * (labels_out == segid)\n  process(extracted_image) # stand in for whatever you'd like to do\n\n# If a read-only image is ok, this approach is MUCH faster\n# if the image has many contiguous regions. A random image\n# can be slower. binary=True yields binary images instead\n# of numbered images.\nfor label, image in cc3d.each(labels_out, binary=False, in_place=True):\n  process(image) # stand in for whatever you'd like to do\n\n# Image statistics like voxel counts, bounding boxes, and centroids.\nstats = cc3d.statistics(labels_out)\n\n# Remove dust from the input image. Removes objects with\n# fewer than `threshold` voxels.\nlabels_out = cc3d.dust(\n  labels_in, threshold=100,\n  connectivity=26, in_place=False\n)\n# Removes objects with &gt;= `threshold` voxels.\nlabels_out = cc3d.dust(labels_in, threshold=100, invert=True)\n# Removes objects with &lt; `threshold[0]` voxels and &gt;= threshold[1]\nlabels_out = cc3d.dust(labels_in, threshold=[50,100])\n# Removes objects with &gt;= `threshold[0]` voxels and &lt; threshold[1]\nlabels_out = cc3d.dust(labels_in, threshold=[50,100], invert=True)\n\n# Get a labeling of the k largest objects in the image.\n# The output will be relabeled from 1 to N.\nlabels_out, N = cc3d.largest_k(\n  labels_in, k=10,\n  connectivity=26, delta=0,\n  return_N=True,\n)\nlabels_in *= (labels_out &gt; 0) # to get original labels\n\n# Compute the contact surface area between all labels.\n# Only face contacts are counted as edges and corners\n# have zero area. To get a simple count of all contacting\n# voxels, set `surface_area=False`.\n# { (1,2): 16 } aka { (label_1, label_2): contact surface area }\nsurface_per_contact = cc3d.contacts(\n  labels_out, connectivity=connectivity,\n  surface_area=True, anisotropy=(4,4,40)\n)\n# same as set(surface_per_contact.keys())\nedges = cc3d.region_graph(labels_out, connectivity=connectivity)\n\n# You can also generate a voxel connectivty graph that encodes\n# which directions are passable from a given voxel as a bitfield.\n# This could also be seen as a method of eroding voxels fractionally\n# based on their label adjacencies.\n# See help(cc3d.voxel_connectivity_graph) for details.\ngraph = cc3d.voxel_connectivity_graph(labels, connectivity=connectivity)\n\n# ...and turn it back into labeled values (probably\n# not exactly the same ones). Note: this function currently\n# assumes an undirected graph, so single voxel alterations are\n# likely to go awry.\nnew_labels = cc3d.color_connectivity_graph(graph, connectivity=connectivity)\n</code></pre> <p>Note: C and Fortran order arrays will be processed in row major and column major order respectively, so the numbering of labels will be \"transposed\". The scare quotes are there because the dimensions of the array will not change.</p>"},{"location":"#c-use","title":"C++ Use","text":"<pre><code>#include \"cc3d.hpp\"\n\n// 3d array represented as 1d array\nint* labels = new int[512*512*512]();\n\nuint32_t* cc_labels = cc3d::connected_components3d&lt;int&gt;(\n  labels, /*sx=*/512, /*sy=*/512, /*sz=*/512\n);\n\n// The default template parameter for output type is uint32_t\nuint64_t* cc_labels = cc3d::connected_components3d&lt;int, uint64_t&gt;(\n  labels, /*sx=*/512, /*sy=*/512, /*sz=*/512\n);\n\nuint16_t* cc_labels = cc3d::connected_components3d&lt;int, uint16_t&gt;(\n  labels, /*sx=*/512, /*sy=*/512, /*sz=*/512,\n  /*connectivity=*/18 // default is 26 connected\n);\n\nsize_t N = 0;\nuint16_t* cc_labels = cc3d::connected_components3d&lt;int, uint16_t&gt;(\n  labels, /*sx=*/512, /*sy=*/512, /*sz=*/512,\n  /*connectivity=*/26, /*N=*/N // writes number of labels to N\n);\n\n#include \"cc3d_continuous.hpp\"\n\n// For handling grayscale images. Note that the difference\n// is the addition of the \"delta\" argument.\nuint16_t* cc_labels = cc3d::connected_components3d&lt;int, uint16_t&gt;(\n  labels, /*sx=*/512, /*sy=*/512, /*sz=*/512,\n  /*delta=*/10, /*connectivity=*/6 // default is 26 connected\n);\n\n#include \"cc3d_graphs.hpp\"\n\n// edges is [ e11, e12, e21, e22, ... ]\nstd::vector&lt;uint64_t&gt; edges = cc3d::extract_region_graph&lt;uint64_t&gt;(\n  labels, /*sx=*/512, /*sy=*/512, /*sz=*/512,\n  /*connectivity=*/18 // default is 26 connected\n);\n\n// graph is a series of bitfields that describe inter-voxel\n// connectivity based on adjacent labels. See \"cc3d_graphs.hpp\"\n// for details on the bitfield.\nuint32_t* graph = extract_voxel_connectivity_graph&lt;T&gt;(\n  labels, /*sx=*/512, /*sy=*/512, /*sz=*/512,\n  /*connectivity=*/6 // default is 26 connected\n);\n</code></pre>"},{"location":"#26-connected-ccl-algorithm","title":"26-Connected CCL Algorithm","text":"<p>The algorithm contained in this package is an elaboration into 3D images of the 2D image connected components algorithm described by Rosenfeld and Pflatz (RP) in 1968 [1] (which is well illustrated by this youtube video) using an equivalency list implemented as Tarjan's Union-Find disjoint set with path compression and balancing [2] and augmented with a decision tree based on work by Wu, Otoo, and Suzuki (WOS), an approach commonly known as Scan plus Array-based Union-Find (SAUF). [3] The description below describes the 26-connected algorithm, but once you understand it, deriving 18 and 6 are simple. However, we recently made some changes that warrant further discursion on 6-connected.</p>"},{"location":"#first-principles-in-2d","title":"First Principles in 2D","text":"<p>In RP's 4-connected two-pass method for binary 2D images, the algorithm raster scans and every time it first encounters a foreground pixel (the pixels to its top and left are background), it marks it with a new label. If there is a preexisting label in its neighborhood, it uses that label instead. Whenever two labels are adjacent, it records they are equivalent so that they can be relabeled consistently in the second pass. This equivalency table can be constructed in several ways, but some popular approaches are Union-Find with path compression with balancing by rank and Selkow's algorithm (which can avoid pipeline stalls). [4] However, Selkow's algorithm is designed for two trees of depth two, appropriate for binary images. We would like to process multiple labels at the same time, making Union-Find preferable.</p> <p>In the second pass, the pixels are relabeled using the equivalency table. Union-Find establishes one label as the root label of a tree, and the root is considered the representative label. Each pixel is then labeled with the representative label. Union-Find is therefore appropriate for representing disjoint sets. Path compression with balancing radically reduces the height of the tree, which accelerates the second pass.</p> <p>WOS approached the problem of accelerating 8-connected 2D connected components on binary images. 8-connected labeling is achieved by extending RP's forward pass mask to the top left and top right corner pixels. In Union-Find based connected components algorithms, the unify step in the first pass is the most expensive step. WOS showed how to optimize away a large fraction of these calls using a decision tree that takes advantage of local topology. For example, since the top-center neighbor of the current pixel is also adjacent to the other mask elements, all of which have already been processed by virtue of the raster scan direction, if it is present it is sufficient to copy its value and move on. If it is absent, pick one of the remaining foreground pixels, copy their value, and use unify for the mask element on the right as it is now known to be non-neighboring with the left hand side. WOS's algorithm continues in this fashion until a match is found or all mask elements are processed at which point a new label is created.</p> <p>For several years, this algorithm was the world's fastest, though it has been superceded by a newer work that exchanges the static decision tree for a dynamic one or precalculated generated one amongst other improvements. However, WOS's work is significant for both its simplicity and speed and thus serves as the inspiration for this library. For 2D 8-connected images, we provide a specialization using Wu et al's original decision tree for a slight performance boost.</p> <p>We're interested in exploring the block based approaches of Grana, Borghesani, and Cucchiara ([5],[7]), however their approach appears to critically rely on binary images. We'll continue to think about ways to incorporate it. We also considered the approach of He et al [8] which is also supposed to modestly faster than than WOS. However, it substitutes the Union-Find data structure (one array) with three arrays, which imposes a memory requirement that is at odds with our goal of processing large images.</p>"},{"location":"#extending-to-3d","title":"Extending to 3D","text":"<p>The approach presented below is very similar to that of Sutheebanjard [6]. To move to a 3D 26-connected neighborhood, the mask must be extended into three dimensions in order to connect neighboring planes. Observe that the 8-connected mask covers the trailing half of the neighborhood (the part that will have been already processed) such that the current pixel can rely on those labels. Thus the mask for the 26-connected neighborhood covers only two out of three potential planes: the entire lower plane (nine voxels), and a mask identical to WOS's (four voxels) on the current plane. While some further optimizations are possible, to begin, the problem can be conceptually decomposed into two parts: establishing a 9-connected link to the bottom plane and then an 8-connected link to the current plane. This works because the current pixel functions as a hub that transmits the connection information from the 9-connected step to the 8-connected step.</p> <p>Fig. 1: Mask for an 8-connected plane. If J,K,L, and M are all eliminated, only N remains and a new label is assigned.</p> j k l m n . . . . <p>The very first Z plane (Z=0) the algorithm runs against is special: the edge effect omits the bottom plane of the mask. Therefore, as the remaining mask is only comprosed of the 8-connected 2D mask, after this pass, the bottom of the image is 8-connected. At Z=1, the 9-connected part of the mask kicks in, forming connections to Z=0, making the current plane now (8 + 9) 17-connected. At Z=2, the 9-connected bottom mask now forms connections from Z=1 to Z=2 on the top, making Z=1 (17 + 9) 26-connected. By induction, when this process proceeds to completion it results in a 26-connected labeling of the volume.</p> <p>Following inspiration from WOS, we construct a decision tree on the densely labeled bottom plane that minimizes the number of unifications we need to perform.</p> <p>Fig 2. The mask for the lower plane in 3D.</p> a b c d e f g h i <p>As <code>e</code> is connected to all other voxels, if present, it can simply be copied. If <code>e</code> is absent, <code>b</code> and <code>h</code> fully cover the mask. If <code>b</code> is absent, <code>h</code>, <code>a</code>, <code>c</code> comprise a covering. If <code>h</code> is absent, <code>b</code>, <code>g</code>, <code>i</code> are one. Below is a list of coverings such that each proceeding entry in the list assumes the first letters in the entries above are background.</p> <ol> <li><code>e</code></li> <li><code>k</code>, (<code>h</code> | <code>g</code>, <code>i</code>)</li> <li><code>b</code>, (<code>h</code> | <code>g</code>, <code>i</code>)</li> <li><code>h</code>, <code>a</code>, <code>c</code></li> <li><code>m</code>, (<code>f</code> | <code>c</code>, <code>i</code>)</li> <li><code>d</code>, (<code>f</code> | <code>c</code>, <code>i</code>)</li> <li><code>f</code>, <code>g</code>, <code>a</code></li> <li><code>a</code>, <code>c</code>, <code>g</code>, <code>i</code></li> <li><code>c</code>, <code>g</code>, <code>i</code></li> <li><code>g</code>, <code>i</code></li> <li><code>i</code></li> </ol> <p>The decision tree is then constructed such that each of these coverings will be evaluated using the fewest unifications possible. It's possible to further optimize this by noting that <code>e</code> and <code>b</code> are both fully connected to the upper 2D mask. Therefore, if either of them are present, we can skip the 8-connected unification step. It's also possible to try the DF covering first if B is background, which would save one unification versus HAC given even statistics, but it seems to be slightly slower on the dataset I attempted. To move from binary data to multilabel data, I simply replaced tests for foreground and background with tests for matching labels.</p> <p>In order to make a reasonably fast implementation, I implemented union-find with path compression. I conservatively used an IDs array qual to the size of the image for the union-find data structure instead of a sparse map. The union-find data structure plus the output labels means the memory consumption will be input + output + rank + equivalences. If your input labels are 32-bit, the memory usage will be 4x the input size. This becomes more problematic when 64-bit labels are used, but if you know something about your data, you can decrease the size of the union-find data structure. I previously used union-by-size but for some reason it merely reduced performance and increased memory usage so it was removed.</p> <p>For more information on the history of connected components algorithms, and an even faster approach for 2D 8-connected components, consult Grana et al's paper on Block Based Decision Trees. [5,7]</p>"},{"location":"#phantom-labels","title":"Phantom Labels","text":"<p>In the course of thinking of improvements to several algorithms, we developed a technique we term \"Phantom Labeling\" for improving the SAUF method directly.</p> <pre><code>Definition: Phantom Labels are elements of a CCL mask that\ntransmit connectivity information between other elements of the\nmask but cannot directly pass their value to the current pixel\nduring the first pass of a SAUF derived algorithm.\n</code></pre> <p>Reproducing Fig. 1 again, but with new letters for the more limited problem, the standard SAUF mask appears like so:</p> <p>Fig. 3: Mask for an 8-connected plane.</p> a b c d x . . . . <p>This results in a decision tree like so assuming x is a foreground pixel.</p> <pre><code>if b:\n    x := b\nelif a:\n    x := a\n    if c:\n        unify(a,c)\nelif d:\n    x := d\n    if c:\n        unify(c,d)\nelif c:\n    x := c\nelse:\n    x := new label\n</code></pre> <p>There is an opportunity here for eliminating up to half of the unify calls, one of the more expensive operations in modern CCL by slightly modifying the mask:</p> <p>Fig. 4: 8-connected mask modified to include phantom label P.</p> . P . a b c d x . . . . <p>This results in a modified decision tree.</p> <pre><code>if b:\n    x := b\nelif a:\n    x := a\n    if c and not P: &lt;--- change here\n        unify(a,c)\nelif d:\n    x := d\n    if c:\n        unify(c,d)\nelif c:\n    x := c\nelse:\n    x := new label\n</code></pre> <p>The novelty of this technique is unclear, but it is very simple to apply and results in substantial speed ups for the 4 and 6 connected problems, a minor improvement for 8-connected, and is readily compatible with the multi-label approach unlike block based approaches.</p>"},{"location":"#4-and-6-connected-ccl-algorithm","title":"4 and 6-Connected CCL Algorithm","text":"<p>Here is where the phantom label technique shines. It's a bit harder to find 4 and 6 connected algorithms in the literature, I assume because many of the techniques invented for the 8-way problem, such as the Union-Find data structure for the equivalency table and run-based approaches, are applicable to the simpler problem. However, the SAUF decision tree approach was lacking as every pixel required a unify call in the 4-way problem and two in the 6-way problem.</p> <p>Fig. 5: 4-connected mask modified to include phantom label P.</p> P b . a x . <pre><code>if a:\n    x := a\n    if b and not P:\n        unify(a,b)\nelif b:\n    x := b\nelse:\n    x := new label\n</code></pre> <p>This gives a decent improvement on the order of 10-20%. If you're lucky, you might not incur even a single label merge operation. In the 6-way problem, there are three phantom labels that can be exploited and the improvement is closer to 50% on our data, a fairly substantial amount. Again, with luck you might avoid any unify operations at all.</p> <p>Fig. 6: Mask for the 6-way problem with phantom labels P, Q, and R added.</p> P b a x . Q R c <p>You can even use multiple routes to propagate information if a label is missing. For example, if path (a,P,b) is unavailable due to a missing P, you could potentially transmit information using path (a,R,c,Q,b).</p>"},{"location":"#four-pass-algorithm","title":"Four Pass Algorithm","text":"<p>We introduce two additional passes over the image label prior to running the two-pass SAUF algorithm. These additional passes are used to collect statistcs for optimizing the SAUF passes.</p>"},{"location":"#estimating-provisional-labels","title":"Estimating Provisional Labels","text":"<p>The first additional pass is used to over-estimate the number of provisional labels generated by the first SAUF pass. A better estimation allows a smaller allocation for the Union-Find datastructure. For some operating systems, the reduced size of the allocation and improved caching recovers more time than is spent collecting statistics.</p> <p>This can be computed by counting the number of transitions between labels along each row of the image. This scan is easily written such that the instructions can be vectorized to minimize the cost of the scan. The number of transitions is guaranteed to be larger than or equal to the number of provisional labels as all provisional labels are generated in this fashion and then reduced by stealing a label from a neighboring voxel.</p> <p>A hierarchy of estimators can be written as:</p> <pre><code>0 &lt;= provisional labels &lt;= X transitions &lt;= static estimate &lt;= voxels\n</code></pre> <p>Binary images can also be estimated statically as <code>voxels / 2</code> for 4 and 6-way, <code>voxels / 4</code> for 8 and 18 way, and <code>voxels / 8</code> for 26 connected. For multi-label images, the best static estimate is <code>voxels</code> as no assumptions can be made about how labels connect to each other (in the worst case all eight voxels in a cube have different labels).</p> <p>It is also possible to check XY and XYZ transitions to get a tighter bound, but in experiments, the amount of time spent checking those directions exceeded the benefit obtained by checking the X pass. Often the X pass alone results in factors as high as <code>voxels / 100</code>.</p> <p>Estimation of the number of labels also allows aborting processing before the first SAUF pass in the case of an all background cube.</p>"},{"location":"#estimating-foreground-location","title":"Estimating Foreground Location","text":"<p>The second additional pass is estimating the location of the foreground. In the literature, this strategy is sometimes referred to as a \"one-and-a-half pass\" where the foreground location is computed during the first SAUF pass and then used to skip processing of background voxels during the relabeling pass.</p> <p>Here we perform this check up front so that it can be performed minimally. Instead of integrating the calculation into the first pass which could force some computation on every voxel, we scan each row from the left to find the first foreground voxel and then scan from the right to the find the foreground voxel at the end. The results are tabulated in a uint32 table of starts and ends to each row of size <code>2 * sy * sz</code>. This ensures that the volume is scanned at most once, and most likely much less if the shapes fill the space reasonably well. Then, both passes of the SAUF method scan only the part of each row indicated by this table.</p> <p>Certain shapes and distributions defeat the efficiency of scanning only the starts and ends of the row (such as random images or an image with foreground on the start and end of each row and nowhere else). However, for a great many shapes, this provides substantial efficiencies and minimal downside for a dense multi-label image as only two YZ slices of the images are scanned before the table is completed.</p>"},{"location":"#early-abortion-points","title":"Early Abortion Points","text":"<p>There are three locations in the algorithm at which further processing can be aborted early without changing the result.</p> <ol> <li>After estimating provisional labels if zero transitions are detected (an all zeros volume). A black image is returned.</li> <li>After the first SAUF pass if the number of provisional labels is zero or one. In this case, the provisional labels are guaranteed to be identical to final labels.</li> <li>After assigning final labels to each provisional label in a translation array. If the number of final labels equals the number of provisional labels, the provisional labels were accurately assigned and the relabeling scan can be skipped.</li> </ol>"},{"location":"#papers-using-cc3d","title":"Papers Using cc3d","text":"<p>A number of papers are using cc3d now. Many of them seem to be deep learning applications as instance segmentation is liable to generate touching non-binary labels. Some are in geoscience, neuroscience, and medical fields. If cc3d is helpful to you, please feel free to email us and let us know. We might be able to offer some tips if its performance critical (though we can't guarantee timeliness of response). There are so many variations of the CCL problem, you might be surprised at what you can do.</p> <p>https://scholar.google.com/scholar?as_ylo=2019&amp;q=connected-components-3d&amp;hl=en&amp;as_sdt=0,31</p>"},{"location":"#references","title":"References","text":"<ol> <li>A. Rosenfeld and J. Pfaltz. \"Sequential Operations in Digital Picture Processing\". Journal of the ACM. Vol. 13, Issue 4, Oct. 1966, Pg. 471-494. doi: 10.1145/321356.321357 (link)</li> <li>R. E. Tarjan. \"Efficiency of a good but not linear set union algorithm\". Journal of the ACM, 22:215-225, 1975. (link)</li> <li>K. Wu, E. Otoo, K. Suzuki. \"Two Strategies to Speed up Connected Component Labeling Algorithms\". Lawrence Berkeley National Laboratory. LBNL-29102, 2005. (link)</li> <li>S. Selkow. \"The Tree-to-Tree Editing Problem\". Information Processing Letters. Vol. 6, No. 6. June 1977. doi: 10.1016/0020-0190(77)90064-3 (link)</li> <li>C. Grana, D. Borghesani, R. Cucchiara. \"Optimized Block-based Connected Components Labeling with Decision Trees\". IEEE Transactions on Image Processing. Vol. 19, Iss. 6. June 2010. doi: 10.1109/TIP.2010.2044963 (link)</li> <li>P. Sutheebanjard. \"Decision Tree for 3-D Connected Components Labeling\". Proc. 2012 International Symposium on Information Technology in Medicine and Education. doi: 10.1109/ITiME.2012.6291402 (link)</li> <li>C. Grana, D. Borghesani, R. Cucchiara. \"Fast Block Based Connected Components Labeling\". Proc. 16th IEEE Intl. Conf. on Image Processing. 2009. doi: 10.1109/ICIP.2009.5413731 (link)</li> <li>L. He, Y. Chao and K. Suzuki, \"A Linear-Time Two-Scan Labeling Algorithm\", IEEE International Conference on Image Processing, vol. 5, pp. 241-244, 2007.</li> </ol>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>Except where noted, these benchmarks were run on a 2.8 GHz Dual-Core Intel Core i7 with 1600 MHz DDR3 RAM. For binary images, we compared the performance of cc3d to the commonly used <code>scipy.ndimage.measurements.label</code> which supports 26-connected binary images. cc3d was designed to efficiently handle multilabel datasets.</p>"},{"location":"benchmarks/#connectomics-data","title":"Connectomics Data","text":"<p>Except where noted, we compared the time and memory performance of both libraries on <code>connectomics.npy</code>, a 512x512x512 voxel cutout of a dense segmentation of a connectomics dataset at a resolution of 32x32x40 nm<sup>3</sup> containing 2523 labels and 3619 connected components.</p> <p>The volume is derived from an early experimental segmentation of pinky40, a predecessor to the now public pinky100_v185 automatic segmentation of mouse brain now available at https://microns-explorer.org/phase1. You need to gzip decompress to recover the uncompressed file: <code>gunzip connectomics.npy.ckl.gz</code></p>"},{"location":"benchmarks/#image-data","title":"Image Data","text":"<p><code>Misc_pollen.npy</code> is the microscopy image from https://en.wikipedia.org/wiki/Scanning_electron_microscope#/media/File:Misc_pollen.jpg which is a public domain photo taken by the Dartmouth College Electron Microscope Facility in 2004.</p>"},{"location":"benchmarks/#multi-label-comparison","title":"Multi-Label Comparison","text":"<p>  Fig. 1: Optimized extraction of components using cc3d 3.1.0 on a 512x512x512 densely labeled connectomics segmentation. </p> <p>  Fig. 2: Extraction of components on a 512x512x512 densely labeled connectomics segmentation. (black) scikit-image 0.19.2 and (blue) cc3d 3.10.0 </p> <pre><code>import cc3d\nimport skimage.measure\nfrom tqdm import tqdm\nimport numpy as np\n\ndef scikit_image_multilabel_extraction(labels):\n  res, N = skimage.measure.label(labels, return_num=True)\n  for segid in tqdm(range(1, N+1)):\n    extracted = (res == segid)\n\ndef cc3d_mutlilabel_extraction(labels):\n  res, N = cc3d.connected_components(labels, return_N=True)\n  for label, extracted in tqdm(cc3d.each(res, in_place=True), total=N):\n    pass\n</code></pre> <p>This is the fastest method we have for running CCL and then extracting each label from the result. CCL takes a fraction of a second, an index of the location of each shape is compiled, and then each shape is then drawn onto an output image and then erased before the next iteration of the loop. Compared with a pure scipy and numpy approach, this method is thousands of times faster if a full sized image is required. Scipy/numpy can perform much better if <code>scipy.ndimage.find_objects</code> and cropping is employed.</p> <p>The memory usage is higher here compared with the figure below due to the size of the location index.</p> <p>  Fig. 2: Extracting components using SciPy vs cc3d on a 512x512x512 densely labeled connectomics segmentation. (black) 20% of SciPy 1.3.0 (blue) 100% of cc3d 1.2.2 </p> <pre><code>import cc3d\nfrom tqdm import tqdm\nimport scipy.ndimage.measurements\nimport numpy as np\nimport fastremap\n\ndef cc3d_test(labels):\n  labels, remap = fastremap.renumber(labels)\n  res = cc3d.connected_components(labels)\n  N = np.max(res)\n  for segid in tqdm(range(1, N+1)):\n    extracted = (res == segid)\n\ndef ndimage_test(labels):\n  s = [\n    [[1,1,1], [1,1,1], [1,1,1]],\n    [[1,1,1], [1,1,1], [1,1,1]],\n    [[1,1,1], [1,1,1], [1,1,1]]\n  ]\n\n  uniques = np.unique(labels)[1:]\n  for segid in tqdm(uniques):\n    extracted = (labels == segid)\n    res, N = scipy.ndimage.measurements.label(extracted, structure=s)\n    for ccid in tqdm(range(1,N+1)):\n      extracted = (res == ccid)\n</code></pre> <p>This benchmark was run an x86_64 3.7 GHz Intel Core i7-4820K CPU @ 3.70GHz with DDR3 1600 MHz RAM.</p> <p>In this test, <code>cc3d_test</code> was run to completion in 225 seconds after loading the image and processing it. <code>ndimage_test</code> was arrested manually after 496 iterations (20%) at 2,745 seconds as <code>tqdm</code> projected over three hours of total running time. SciPy's algorithm wins on memory pressure at about 1.7 GB peak usage versus cc3d using about 1.8 to 1.9 GB. SciPy performs poorly here because it must be run thousands of times after masking to expose individual labels since it only supports binary data. SciPy's average iteration per label takes about 5.1 sec. It then must extract the individual components from the results of connected components, but this is fast. By contrast, since <code>cc3d</code> has native multi-label support, it needs to only be run once, with the bulk of time spent querying the resulting image for components.</p>"},{"location":"benchmarks/#10x-head-to-head-connectomics-data","title":"10x Head to Head: Connectomics Data","text":"<p>  Fig. 3: SciPy vs cc3d run ten times on a 512x512x512 connectomics segmentation masked to only contain one label. (blue) SciPy 1.6.0 (black) cc3d 3.1.0 </p> <pre><code>import cc3d\nfrom tqdm import tqdm\nimport scipy.ndimage.measurements\nimport numpy as np\n\nlabels = ...\nlabels == labels == 28792756\n\ns = [\n  [[1,1,1], [1,1,1], [1,1,1]],\n  [[1,1,1], [1,1,1], [1,1,1]],\n  [[1,1,1], [1,1,1], [1,1,1]]\n]\n\nfor i in tqdm(range(10)):\n  # cc3d.connected_components(labels)\n  scipy.ndimage.measurements.label(labels, structure=s)\n</code></pre> <p>This comparison was performed to show what happens when SciPy and <code>cc3d</code> are run on realistic single-label data. <code>cc3d</code> performs each iteration in 0.27 seconds while SciPy takes about 5.7 seconds. While in previous versions, cc3d used many times more memory than scipy in this experiment, as of version 2.0.0, the memory usage is now better than SciPy due to estimating the necessary number of provisional labels before executing.</p> Trial MVx/sec Rel. Perf. SciPy 1.6.0 23.7 1.00x cc3d 3.1.0 502.5 21.2x"},{"location":"benchmarks/#10x-head-to-head-random-binary-images","title":"10x Head to Head: Random Binary Images","text":"<pre><code>import numpy as np\nimport scipy.ndimage.measurements\nimport cc3d\n\ns = [\n  [[1,1,1], [1,1,1], [1,1,1]],\n  [[1,1,1], [1,1,1], [1,1,1]],\n  [[1,1,1], [1,1,1], [1,1,1]]\n]\n\nlabels = np.random.randint(0,2, size=(384, 384, 384), dtype=np.bool)\n\nfor label in labels:\n  # scipy.ndimage.measurements.label(label, structure=s) # black\n  cc3d.connected_components(label) # blue\n</code></pre>"},{"location":"benchmarks/#26-connected","title":"26-connected","text":"<p>  Fig. 4: SciPy vs cc3d run ten times on a 384x384x384 random binary image. (black) SciPy 1.6.0 (blue) cc3d 3.1.0 </p> <p>On random binary images, SciPy marginally wins on memory with a peak memory cosumption of 337 MB vs. cc3d with a peak consumption of about 363 MB (1.08x). However, SciPy doesn't perform as well as cc3d in running time with an average run time of 2.45 sec versus 0.92 sec per label set.</p> Trial MVx/sec MB/sec Rel. Perf. SciPy 1.6.0 23.1 23.1 1.0x cc3d 3.1.0 61.7 61.7 2.6x"},{"location":"benchmarks/#6-connected","title":"6-connected","text":"<p>  Fig. 4: SciPy vs cc3d run ten times on a 384x384x384 random binary image using 6-connectivity. (black) SciPy 1.5.2 (blue) cc3d 1.13.0 </p> <p>SciPy uses about 390 MB, which is oddly increasing, while cc3d uses 405 MB. SciPy averages 1.31 seconds per volume vs cc3d averages 0.95 seconds per volume.</p> Trial MVx/sec MB/sec Rel. Perf. SciPy 1.5.2 43.4 43.4 1.0x cc3d 1.13.0 59.9 59.9 1.4x"},{"location":"benchmarks/#10x-head-to-head-black-cube","title":"10x Head to Head: Black Cube","text":"<p>  Fig. 4: Different configurations run against a uint64 512x512x512 black cube using 26-connectivity. (blue) SciPy 1.6.0 (black) cc3d 3.1.0 </p> <p>Sometimes empty data shows up in your pipeline. Sometimes a lot of it. How do your libraries handle it? At full speed? Slower? Faster than normal?</p> <p>Here we show scipy versus cc3d using 26 connectivity. cc3d will skip the relabeling pass if provisional labels total fewer than two. It will also skip the decision tree pass and memory allocation of data structures as well if it estimates zero provisional voxels.</p> <p>We can see how this bears out. In black, SciPy runs at a brisk and reasonable clip. In data not shown, it appears to have some optimization for black voxels as it runs more slowly on a solid color non-zero cube. cc3d skips everything except the scan for foreground labels.</p> Trial MVx/sec Rel. Perf. SciPy 1.6.0 102 1.0x cc3d 3.1.0 923 9.0x"},{"location":"benchmarks/#historical-performance","title":"Historical Performance","text":"<p>cc3d has been steadily improving over time. To celebrate the release of 2.0.0, we show plots of peak memory usage and megavoxels per second vs version. Better scores in these charts trend down and right, indicating lower peak memory pressure and faster execution. In May 2022, I discovered that scikit-image also is multi-label capable and so added comparisons to these charts.</p> <p>  Fig. 6: 26-way cc3d peak memory usage and speed in selected releases against a 512x512x512 connectomics dataset. </p> <p>  Fig. 7: 26-way cc3d and scipy peak memory usage and speed in selected releases against a 512x512x512 random binary dataset. </p> <p>  Fig. 8: 26-way cc3d peak memory usage and speed against a natural binary image from a 512x512x512 connectomics dataset. </p> <p>  Fig. 9: 6-way cc3d peak memory usage and speed in selected releases against a 512x512x512 connectomics dataset. </p>"},{"location":"docs/api/","title":"API","text":""},{"location":"docs/api/#cc3d.DimensionError","title":"DimensionError","text":"<p>               Bases: <code>Exception</code></p> <p>The array has the wrong number of dimensions.</p>"},{"location":"docs/api/#cc3d.color_connectivity_graph","title":"color_connectivity_graph","text":"<pre><code>color_connectivity_graph(\n    vcg: NDArray[VcgT],\n    connectivity: Literal[4, 6, 8, 18, 26] = 26,\n    return_N: bool = False,\n) -&gt; Union[NDArray[VcgT], tuple[NDArray[VcgT], int]]\n</code></pre> <p>Color the connectivity graph of a voxel connectivity graph.</p> <p>Given a voxel connectivity graph following the same bit convention as <code>cc3d.voxel_connectivity_graph</code> (see docstring), assuming an undirected graph (the format supports directed graphs, but this is not implemented for the sake of efficiency), this function will return a uint32 image that contains connected components labeled according to the boundaries described in the voxel connectivity graph (vcg).</p>"},{"location":"docs/api/#cc3d.connected_components","title":"connected_components","text":"<pre><code>connected_components(\n    data: NDArray[Any],\n    max_labels: int = -1,\n    connectivity: Literal[4, 6, 8, 18, 26] = 26,\n    return_N: bool = False,\n    delta: float = 0,\n    out_dtype: DTypeLike = None,\n    out_file: Union[str, BinaryIO, None] = None,\n    periodic_boundary: bool = False,\n    binary_image: bool = False,\n) -&gt; Union[\n    NDArray[Union[np.uint16, np.uint32, np.uint64]],\n    tuple[NDArray[Union[np.uint16, np.uint32, np.uint64]], int],\n]\n</code></pre> <p>Connected components applied to 3D images with handling for multiple labels.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>NDArray[Any]</code> <p>Input weights in a 2D or 3D numpy array.</p> required <code>max_labels</code> <code>int</code> <p>Save memory by predicting the maximum number of possible labels that might be output. Defaults to number of voxels.</p> <code>-1</code> <code>connectivity</code> <code>int</code> <p>For 3D images, 6 (voxel faces), 18 (+edges), or 26 (+corners) If the input image is 2D, you may specify 4 (pixel faces) or 8 (+corners).</p> <code>26</code> <code>return_N</code> <code>bool</code> <p>If True, also return the number of connected components as the second argument of a return tuple.</p> <code>False</code> <code>delta</code> <code>same as data</code> <p>Must be greater than or equal 0. Connect together values whose difference in value is &lt;= delta. Useful for rough segmentations of continuously valued images.</p> <code>0</code> <code>out_dtype</code> <code>DTypeLike</code> <p>If specified, must be one of np.uint16, np.uint32, np.uint64. If not specified, it will be automatically determined. Most of the time, you should leave this off so that the smallest safe dtype will be used. However, in some applications you can save an up-conversion in the next operation by outputting the appropriately sized type instead.</p> <code>None</code> <code>out_file</code> <code>Union[str, BinaryIO, None]</code> <p>If specified, the output array will be an mmapped file. Can be a file-name or a file-like object.</p> <code>None</code> <code>periodic_boundary</code> <code>bool</code> <p>The boundary edges wrap around.</p> <code>False</code> <code>binary_image</code> <code>bool</code> <p>If True, regardless of the input type, treat as a binary image (foreground &gt; 0, background == 0). Certain inputs will always be treated as a binary image (e.g. bool dtype, delta == max int or max float etc.).</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[NDArray[Union[uint16, uint32, uint64]], tuple[NDArray[Union[uint16, uint32, uint64]], int]]</code> <p>Returns either a tuple <code>(out, N)</code> if return_N is True, or just <code>out</code> otherwise, where <code>out</code> is a 1D, 2D, or 3D NumPy array representing the input remapped to reflect connected components sequentially numbered from 1 to <code>N</code>. The data type of <code>out</code> is automatically selected as <code>uint16</code>, <code>uint32</code>, or <code>uint64</code> based on the estimated number of provisional labels required. <code>N</code> denotes the total number of connected components identified.</p>"},{"location":"docs/api/#cc3d.connected_components_stack","title":"connected_components_stack","text":"<pre><code>connected_components_stack(\n    stacked_images: Iterable[NDArray[Any]],\n    connectivity: Literal[6, 26] = 26,\n    return_N: bool = False,\n    binary_image: bool = False,\n) -&gt; Union[CrackleArray, tuple[CrackleArray, int]]\n</code></pre> <p>This is for performing connected component labeling on an array larger than RAM.</p> <p>stacked_images is a sequence of 3D images that are of equal width and height (x, y) and arbitrary depth (z). For example, you might define a generator that produces a tenth of your data at a time. The data must be sequenced in z order from <code>z = 0</code> to <code>z = depth - 1</code>.</p> <p>Each 3D image will have CCL run on it and then compressed into crackle format (https://github.com/seung-lab/crackle) which is highly compressed but still usable and randomly accessible by z-slice. </p> <p>The bottom previous slice and top current slice will be analyzed to produce a merged image.</p> <p>The final output will be a CrackleArray. You can access parts of the image using standard array operations, write the array data to disk using <code>arr.binary</code> or fully decompressing the array using <code>arr.decompress()</code> to obtain a numpy array (but presumably this will blow out your RAM since the image is so big).</p> <p>Parameters:</p> Name Type Description Default <code>stacked_images</code> <code>Iterable[NDArray[Any]]</code> <p>A sequence of images to process.</p> required <code>connectivity</code> <code>Literal[6, 26]</code> <p>(2d) 6 [faces], 26 [faces + edges + corners]</p> <code>26</code> <code>return_N</code> <code>bool</code> <p>Change return value to (CrackleArray, N).</p> <code>False</code> <code>binary_image</code> <code>bool</code> <p>Treat the input images as binary images.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[CrackleArray, tuple[CrackleArray, int]]</code> <p>A CrackleArray containing the connected components</p> <code>Union[CrackleArray, tuple[CrackleArray, int]]</code> <p>of the input images and optionally the number of</p> <code>Union[CrackleArray, tuple[CrackleArray, int]]</code> <p>connected components in the image.</p>"},{"location":"docs/api/#cc3d.contacts","title":"contacts","text":"<pre><code>contacts(\n    labels: NDArray[Any],\n    connectivity: Literal[4, 6, 8, 18, 26] = 26,\n    surface_area: bool = True,\n    anisotropy: tuple[\n        Union[int, float], Union[int, float], Union[int, float]\n    ] = (1, 1, 1),\n) -&gt; dict[tuple[int, int], Union[int, float]]\n</code></pre> <p>Get the N-connected region adjacancy graph of a 3D image and the contact area between two regions.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>NDArray[Any]</code> <p>3D numpy array of integer segmentation labels.</p> required <code>connectivity</code> <code>Literal[4, 6, 8, 18, 26]</code> <p>6, 18, or 26 (default).</p> <code>26</code> <code>surface_area</code> <code>bool</code> <p>Should the returned value be the contact surface area or a simple count of neighboring voxels? Surface area only counts face contact as edges and corners have zero area.</p> <code>True</code> <code>anisotropy</code> <code>tuple[Union[int, float], Union[int, float], Union[int, float]]</code> <p>Weights for x, y, and z dimensions for computing surface area.</p> <code>(1, 1, 1)</code> <p>Returns:</p> Type Description <code>dict[tuple[int, int], Union[int, float]]</code> <p>A dictionary resembling <code>{ (label_1, label_2): float, ... }</code>.</p>"},{"location":"docs/api/#cc3d.dust","title":"dust","text":"<pre><code>dust(\n    img: NDArray[Any],\n    threshold: Union[\n        int, float, tuple[int, int], tuple[float, float], list[int], list[float]\n    ],\n    connectivity: Literal[4, 6, 8, 18, 26] = 26,\n    in_place: bool = False,\n    binary_image: bool = False,\n    precomputed_ccl: bool = False,\n    invert: bool = False,\n    return_N: bool = False,\n) -&gt; Union[NDArray[typing.Any], tuple[NDArray[typing.Any], int]]\n</code></pre> <p>Remove from the input image connected components smaller than threshold (\"dust\").</p> <p>The name of the function can be read as a verb \"to dust\" the image.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>NDArray[Any]</code> <p>A 2D or 3D image.</p> required <code>threshold</code> <code>Union[int, float, tuple[int, int], tuple[float, float], list[int], list[float]]</code> <p>(int) discard components smaller than this in voxels (tuple/list) discard components outside this range [lower, upper)</p> required <code>connectivity</code> <code>Literal[4, 6, 8, 18, 26]</code> <p>A cc3d connectivity to use.</p> <code>26</code> <code>in_place</code> <code>bool</code> <p>Whether to modify the input image or perform dust.</p> <code>False</code> <code>precomputed_ccl</code> <code>bool</code> <p>For performance, avoid computing a CCL pass since the input is already a CCL output from this library.</p> <code>False</code> <code>invert</code> <code>bool</code> <p>Switch the threshold direction. For scalar input, this means less than converts to greater than or equal to, for ranged input, switch from between to outside of range.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[NDArray[Any], tuple[NDArray[Any], int]]</code> <p>The dusted image.</p>"},{"location":"docs/api/#cc3d.each","title":"each","text":"<pre><code>each(\n    labels: NDArray[UnsignedIntegerT],\n    binary: bool = False,\n    in_place: bool = False,\n) -&gt; Iterator[tuple[int, NDArray[UnsignedIntegerT]]]\n</code></pre> <p>Returns an iterator that extracts each label from a dense labeling.</p> <p>Parameters:</p> Name Type Description Default <code>binary</code> <code>bool</code> <p>Create a binary image from each component (otherwise use the same dtype and label value for the mask).</p> <code>False</code> <code>in_place</code> <code>bool</code> <p>Much faster but the resulting image will be read-only.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; for label, img in cc3d.each(labels, binary=False, in_place=False):\n        process(img)\n</code></pre> <p>Returns:</p> Type Description <code>Iterator[tuple[int, NDArray[UnsignedIntegerT]]]</code> <p>An iterator.</p>"},{"location":"docs/api/#cc3d.estimate_provisional_labels","title":"estimate_provisional_labels","text":"<pre><code>estimate_provisional_labels(data: NDArray[Any]) -&gt; tuple[int, int, int]\n</code></pre> <p>Estimate the number of provisional labels required for connected components.</p>"},{"location":"docs/api/#cc3d.largest_k","title":"largest_k","text":"<pre><code>largest_k(\n    img: NDArray[Any],\n    k: int,\n    connectivity: Literal[4, 6, 8, 18, 26] = 26,\n    delta: Union[int, float] = 0,\n    return_N: bool = False,\n    binary_image: bool = False,\n    precomputed_ccl: bool = False,\n) -&gt; Union[\n    NDArray[Union[np.bool_, np.uint16, np.uint32, np.uint64]],\n    tuple[NDArray[Union[np.bool_, np.uint16, np.uint32, np.uint64]], int],\n]\n</code></pre> <p>Returns the k largest connected components in the image.</p> NOTE <p>Performance may increase if you have the <code>fastremap</code> library installed. This may also change the numbering of the output.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>The number of components to return (&gt;= 0).</p> required <code>connectivity</code> <code>Literal[4, 6, 8, 18, 26]</code> <p>(2d) 4 [edges], 8 [edges + corners]  (3d) 6 [faces], 18 [faces + edges], or 26 [faces + edges + corners]</p> <code>26</code> <code>delta</code> <code>Union[int, float]</code> <p>If using a continuous image, the allowed difference in adjacent voxel values.</p> <code>0</code> <code>return_N</code> <code>bool</code> <p>Change return value to (image, N).</p> <code>False</code> <code>binary_image</code> <code>bool</code> <p>Treat the input image as a binary image.</p> <code>False</code> <code>precomputed_ccl</code> <code>bool</code> <p>For performance, avoid computing a CCL pass since the input is already a CCL output from this library.</p> <code>False</code>"},{"location":"docs/api/#cc3d.region_graph","title":"region_graph","text":"<pre><code>region_graph(\n    labels: NDArray[integer], connectivity: Literal[4, 6, 8, 18, 26] = 26\n) -&gt; set[tuple[int, int]]\n</code></pre> <p>Get the N-connected region adjacancy graph of a 3D image.</p> <p>For backwards compatibility. \"contacts\" may be more useful.</p> <p>Supports 26, 18, and 6 connectivities.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>NDArray[integer]</code> <p>3D numpy array of integer segmentation labels.</p> required <code>connectivity</code> <code>Literal[4, 6, 8, 18, 26]</code> <p>6, 18, or 26 (default).</p> <code>26</code> <p>Returns:</p> Type Description <code>set[tuple[int, int]]</code> <p>A set of edges between labels.</p>"},{"location":"docs/api/#cc3d.statistics","title":"statistics","text":"<pre><code>statistics(\n    out_labels: NDArray[Any], no_slice_conversion: bool = False\n) -&gt; Union[StatisticsDict, StatisticsSlicesDict]\n</code></pre> <p>Compute basic statistics on the regions in the image.</p> <p>These are the voxel counts per label, the axis-aligned bounding box, and the centroid of each label.</p> LIMITATION <p>Input must be &gt;=0 and &lt; num voxels.</p> <p>Parameters:</p> Name Type Description Default <code>out_labels</code> <code>NDArray[Any]</code> <p>A numpy array of labels.</p> required <code>no_slice_conversion</code> <code>bool</code> <p>if True, return the bounding_boxes as a numpy array. This can save memory and time.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[StatisticsDict, StatisticsSlicesDict]</code> <p>A dictionary with the following structure.</p> <pre><code>N = np.max(out_labels)\n# Index into array is the CCL label.\n{\n    voxel_counts: NDArray[np.uint32], # (index is label) (N+1)\n    # Structure is xmin, xmax, ymin, ymax, zmin, zmax by label\n    bounding_boxes: NDArray[np.uint16] | list[tuple(slice, slice, slice)],\n    # Index into list is the connected component ID, the tuple of\n    # slices can be directly used to extract the region of interest\n    # from out_labels using slice notation.\n    # Structure is x,y,z\n    centroids: NDArray[np.float64], # (N+1,3)\n}\n</code></pre>"},{"location":"docs/api/#cc3d.voxel_connectivity_graph","title":"voxel_connectivity_graph","text":"<pre><code>voxel_connectivity_graph(\n    data: NDArray[IntegerT], connectivity: Literal[4, 6, 8, 18, 26] = 26\n) -&gt; NDArray[IntegerT]\n</code></pre> <p>Extracts the voxel connectivity graph from a multi-label image.</p> <p>A voxel is considered connected if the adjacent voxel is the same label.</p> <p>This output is a bitfield that represents a directed graph of the allowed directions for transit between voxels. If a connection is allowed, the respective direction is set to 1 else it set to 0.</p> <p>For 2D connectivity, the output is an 8-bit unsigned integer.</p> <pre><code>Bits 1-4: edges     (4,8 way)\n     5-8: corners   (8 way only, zeroed in 4 way)\n</code></pre> <pre><code>    8       7      6      5      4      3      2      1\n------ ------ ------ ------ ------ ------ ------ ------\n  -x-y    x-y    -xy     xy     -x     +y     -x     +x\n</code></pre> <p>For a 3D 26 and 18 connectivity, the output requires 32-bit unsigned integers, for 6-way the output are 8-bit unsigned integers.</p> <pre><code>Bits 1-6: faces     (6,18,26 way)\n    7-19: edges     (18,26 way)\n   18-26: corners   (26 way)\n   26-32: unused (zeroed)\n</code></pre> <p>6x unused, 8 corners, 12 edges, 6 faces</p> <pre><code>    32     31     30     29     28     27     26     25     24     23\n------ ------ ------ ------ ------ ------ ------ ------ ------ ------\nunused unused unused unused unused unused -x-y-z  x-y-z -x+y-z +x+y-z\n    22     21     20     19     18     17     16     15     14     13\n------ ------ ------ ------ ------ ------ ------ ------ ------ ------\n-x-y+z +x-y+z -x+y+z    xyz   -y-z    y-z   -x-z    x-z    -yz     yz\n    12     11     10      9      8      7      6      5      4      3\n------ ------ ------ ------ ------ ------ ------ ------ ------ ------\n   -xz     xz   -x-y    x-y    -xy     xy     -z     +z     -y     +y\n     2      1\n------ ------\n    -x     +x\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>NDArray[IntegerT]</code> <p>A numpy array.</p> required <code>connectivity</code> <code>Literal[4, 6, 8, 18, 26]</code> <p>The connectivity to use.</p> <code>26</code> <p>Returns:</p> Type Description <code>NDArray[IntegerT]</code> <p>A uint8 or uint32 numpy array the same size as the input.</p>"}]}